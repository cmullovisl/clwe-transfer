accum_count:
- 2
adam_beta2: 0.9995
batch_size: 8096
batch_type: tokens
decay_method: linear
dropout:
- 0.1
early_stopping: 0
feat_merge: concat
feat_vec_exponent: 0.7
feat_vec_size: -1
keep_checkpoint: -1
label_smoothing: 0.1
lambda_vmf: 0.2
learning_rate: 1.0
learning_rate_decay: 0.5
loss: nllvmf
loss_scale: 0
max_generator_batches: 2
max_grad_norm: 5.0
min_lr: 1.0e-09
normalization: tokens
optim: radam
pool_factor: 8192
report_every: 250
reset_optim: all
save_checkpoint_steps: 1000
share_decoder_embeddings: 'true'
share_embeddings: 'true'
train_steps: 1500
truncated_decoder: 0
valid_steps: 250
warmup_end_lr: 0.0007
warmup_init_lr: 1.0e-08
warmup_steps: 4000
weight_decay: 1.0e-05
modify_opts: 'true'
denoise: 'true'
freeze_encoder: 'true'
