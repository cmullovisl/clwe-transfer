vars:
  - stage: newlang
  - baselanguages_list: ["en", "de", "es", "fr", "it"]
  - baselanguages: "en de es fr it"
  - newlanguage: "pt"
  - clwepivot: "en"
  - embdim: 300
  - model: "continuous"
  - baseconfig: "train-base"
  - basemodel: "models/base_avg.pt"
  - autoencoderconfig: "train"
  - backtranslationconfig: "train"

stages:
  # set_stage "newlang"
  #download_embeddings:
  #  cmd: download_embeddings "${newlanguage}"
  #  wdir: "embeddings/models"
  #  deps:
  #  - "/dev/null"
  #  outs:
  #  - "./"
  download_embeddings:
    cmd: download_embeddings "${newlanguage}"
    wdir: "embeddings/models"
    deps: []
    #- "/dev/null"
    outs:
    - "cc.${newlanguage}.${embdim}.vec"
    - "cc.${newlanguage}.${embdim}.bin"

  compute_alignments:
    cmd: compute_alignments "${clwepivot}" "${newlanguage}"
    wdir: "embeddings"
    deps:
    - "models/cc.${newlanguage}.${embdim}.vec"
    outs:
    #- "dicts/"
    - "align/aligned.${newlanguage}.vec-mat"

  extract_data:
    cmd: extract_data "${newlanguage}"
    wdir: "data"
    deps:
    - "ted_talks.tar.gz"
    outs:
    - "extracted/train.${newlanguage}"
    - "extracted/dev.${newlanguage}"
    - "extracted/test.${newlanguage}"

  prepare_monolingual_data:
    cmd: dsets="train dev" prepare_monolingual_data "${newlanguage}"
    wdir: "data"
    deps:
    - "extracted/train.${newlanguage}"
    - "extracted/dev.${newlanguage}"
    - "extracted/test.${newlanguage}"
    outs:
    - "monolingual/"

  build_embeddings:
    cmd: build_embeddings "${newlanguage}"
    deps:
    #- "extract-words.py"
    - "data/monolingual/train.${newlanguage}"
    - "embeddings/align/aligned.${newlanguage}.vec-mat"
    - "embeddings/models/cc.${newlanguage}.${embdim}.vec"
    - "embeddings/models/cc.${newlanguage}.${embdim}.bin"
    outs:
    - "data/embeddings/embeddings.${newlanguage}.vec"

  # set_stage "blindenc"
  prepare_parallel_data:
    foreach: ${baselanguages_list}
    do:
      cmd: prepare_parallel_data "test" "${item} ${newlanguage}"
      wdir: "data"
      deps:
      - "extracted/"
      outs:
      #- "parallel/"
      - "parallel/test.${item}-${newlanguage}.${newlanguage}"
      - "parallel/test.${item}-${newlanguage}.${item}"
      - "parallel/test.${newlanguage}-${item}.${newlanguage}"
      - "parallel/test.${newlanguage}-${item}.${item}"

  evauate_bleu_blindenc:
    foreach: ${baselanguages_list}
    do:
      cmd: ln -sf "${basemodel}" "models/blindenc_avg.pt" && evauate_bleu "blindenc" "${newlanguage}" "${item}"
      deps:
      #- "models/blindenc_avg.pt"
      - "${basemodel}"
      #- "data/eval/"
      #- "data/parallel/"
      - "data/parallel/test.${newlanguage}-${item}.${newlanguage}"
      - "data/parallel/test.${newlanguage}-${item}.${item}"
      - "data/embeddings/"
      outs:
      #- "translations/blindenc/"
      - "translations/blindenc/test.${newlanguage}-${item}.${item}.pred"
      #- "scores/blindenc/"
      - "scores/blindenc/test.${newlanguage}-${item}.jsonl"

  evauate_bleu_blinddec:
    foreach: ${baselanguages_list}
    do:
      cmd: ln -sf "${basemodel}" "models/blinddec_avg.pt" && evauate_bleu "blinddec" "${baselanguages}" "${newlanguage}"
      deps:
      #- "models/blinddec_avg.pt"
      - "${basemodel}"
      #- "data/eval/"
      #- "data/parallel/"
      - "data/parallel/test.${item}-${newlanguage}.${newlanguage}"
      - "data/parallel/test.${item}-${newlanguage}.${item}"
      - "data/embeddings/"
      outs:
      #- "translations/blinddec/"
      - "translations/blinddec/test.${newlanguage}-${item}.${item}.pred"
      #- "scores/blinddec/"
      - "scores/blinddec/test.${newlanguage}-${item}.jsonl"

  # set_stage "autoencoder"
  build_newlang_vocab:
    cmd: build_newlang_vocab "autoencoder" "${basemodel}" "${newlanguage}" "${newlanguage}"
    deps:
    - "${basemodel}"
    - "data/embeddings/"
    outs:
    - "saves/autoencoder.vocab.pt"
    #- "saves/autoencoder.specials.pt"

  concat_autoencoding_corpus:
    cmd: concat_autoencoding_corpus "autoencoder" "${newlanguage}"
    wdir: "data"
    deps:
    - "monolingual/"
    outs:
    #- "corpus/"
    - "corpus/train.autoencoder.src"
    - "corpus/train.autoencoder.tgt"
    - "corpus/dev.autoencoder.src"
    - "corpus/dev.autoencoder.tgt"

  preprocess_reuse_vocab:
    cmd: preprocess_reuse_vocab "autoencoder" "saves/data.vocab.pt"
    deps:
    - "data/corpus/train.autoencoder.src"
    - "data/corpus/train.autoencoder.tgt"
    - "data/corpus/dev.autoencoder.src"
    - "data/corpus/dev.autoencoder.tgt"
    - "saves/data.vocab.pt"
    outs:
    - "saves/autoencoder/"

  train_continue_autoencode:
    cmd: train_continue "autoencoder" "${model}" "${autoencoderconfig}" "${basemodel}"
    deps:
    - "saves/autoencoder/"
    - "${basemodel}"
    outs:
    - "models/autoencoder/"
    metrics:
    - "logs/autoencoder/"

  average_models_autoencode:
    cmd: onmt_average_models -m models/autoencoder/*.pt -o models/autoencoder_avg.pt
    deps:
    - "models/autoencoder/"
    outs:
    - "models/autoencoder_avg.pt"

  evauate_bleu_autoencode:
    foreach: ${baselanguages_list}
    do:
      cmd: evauate_bleu "autoencoder" "${item}" "${newlanguage}"
      deps:
      - "models/autoencoder_avg.pt"
      - "data/parallel/test.${item}-${newlanguage}.${newlanguage}"
      - "data/parallel/test.${item}-${newlanguage}.${item}"
      outs:
      - "translations/autoencoder/test.${item}-${newlanguage}.${newlanguage}.pred"
      - "scores/autoencoder/test.${item}-${newlanguage}.jsonl"

  # set_stage "backtranslate"
  prepare_backtranslation_data_round1:
    cmd: prepare_backtranslation_data "monolingual" "bt1/src" "${baselanguages}" "${newlanguage}"
    wdir: "data"
    deps:
    - "monolingual/"
    outs:
    - "bt1/src/"

  backtranslation_round_round1:
    cmd: freezeenc=y backtranslation_round "${basemodel}" "${basemodel}" "${baselanguages}" "${newlanguage}" "bt1"
    deps:
    - "data/bt1/src/"
    # baselanguage embeddings should also be depended on
    - "data/embeddings/embeddings.${newlanguage}.vec"
    - "${basemodel}"
    - "config/bt1/${backtranslationconfig}.yml"
    outs:
    - "data/corpus/train.bt1.src"
    - "data/corpus/train.bt1.tgt"
    - "data/corpus/dev.bt1.src"
    - "data/corpus/dev.bt1.tgt"
    - "models/bt1_avg.pt"
    - "saves/bt1.vocab.pt"

  evauate_bleu_backtranslation_round1:
    foreach: ${baselanguages_list}
    do:
      cmd: evauate_bleu "bt1" "models/bt1_avg.pt" "${item}" "${newlanguage}"
      deps:
      - "data/embeddings/embeddings.${newlanguage}.vec"
      - "data/parallel/test.${item}-${newlanguage}.${newlanguage}"
      - "data/parallel/test.${item}-${newlanguage}.${item}"
      - "models/bt1_avg.pt"
      outs:
      - "translations/bt1/test.${item}-${newlanguage}.${newlanguage}.pred"
      - "scores/bt1/test.${item}-${newlanguage}.jsonl"

  prepare_backtranslation_data_round2:
    cmd: prepare_backtranslation_data "monolingual" "${newlanguage}" "${baselanguages}"
    wdir: "data"
    deps:
    - "monolingual/"
    outs:
    - "bt/src/round2/"

  backtranslation_round_round2:
    cmd: freezeenc= backtranslation_round "$basemodel" "$btmodel" "${newlanguage}" "${baselanguages}"
    deps:
    - "data/bt/src/round2/"
    - "models/bt/round1/"
    - "models/bt/basemodel/"
    outs:
    #- "data/bt/tgt/round2/"
    - "models/bt/round2/"

  evauate_bleu_backtranslation_round2:
    cmd: evauate_bleu "backtranslate" "$btmodel" "${newlanguage}" "${baselanguages}"
    deps:
    - "models/bt/round2/"
    - "models/backtranslate/round2"
    - "data/eval/"
    outs:
    - "translations/backtranslate/round2/"
    - "scores/backtranslate/round2/"
